{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "646fd80d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Active Campaign - Contacts Report.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-5ecf7a189e2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[1;31m# Conduct A Count of Postcode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m \u001b[0mprint_only_completed_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;31m#produce_data_frame()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-5ecf7a189e2a>\u001b[0m in \u001b[0;36mprint_only_completed_results\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_only_completed_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m   \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Active Campaign - Contacts Report.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unicode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Active Campaign - Contacts Report.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import numpy\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Constants\n",
    "number_of_contacts = 9000\n",
    "records_per_query = 100\n",
    "list_of_site_names = [\"Wimbletech\",\"TheWorkary - Chiswick\",\"Rivertech\",\"TheWorkary - Brompton\",\"TheWorkary - NottingHill\",\"TheWorkary - Hanwell\",\"TheWorkary - Chelsea\",\"TheWorkary - Welling\",\"TheWorkary - Caterham\",\"TheWorkary - Maidenhead\", \"TheWorkary - Barnet Pop Up\",\"TheWorkary - Avonmore\",\"TheWorkary - Rainham\"]\n",
    "list_of_site_postcodes = [\"SW197NB\",\"W42AB\",\"WD31HP\",\"SW50BS\",\"W24EW\",\"W71PD\",\"SW35EZ\",\"DA163PA\",\"CR36TR\",\"SL61JX\",None,\"W148TG\",\"RM139YJ\"]\n",
    "postcode_table = pandas.DataFrame({'SiteName':list_of_site_names,'SitePostcode':list_of_site_postcodes})\n",
    "\n",
    "postcode_regex_filter = \"^(([A-Z]|[a-z]){1,2}[0-9]{1,2}([A-Z]|[a-z]){0,1}\\s{0,1}[0-9]{1}([A-Z]|[a-z]){2})$\"\n",
    "compiled_postcode_regex = re.compile(postcode_regex_filter)\n",
    "\n",
    "# Calculated Haversine Straight Line Distance in Kilometers\n",
    "def calculate_km_distance_between_points(lat1,lon1,lat2,lon2):\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = numpy.sin(dlat/2)**2 + numpy.cos(lat1) * numpy.cos(lat2) * numpy.sin(dlon/2)**2\n",
    "    c = 2 * numpy.arcsin(numpy.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    km_distance = r*c\n",
    "    \n",
    "    return numpy.round(km_distance)\n",
    "\n",
    "# Calculated Haversine Straight Line Distance in Miles\n",
    "def calculate_miles_distance_between_points(lat1,lon1,lat2,lon2):\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(numpy.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = numpy.sin(dlat/2)**2 + numpy.cos(lat1) * numpy.cos(lat2) * numpy.sin(dlon/2)**2\n",
    "    c = 2 * numpy.arcsin(numpy.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    km_distance = r*c\n",
    "    \n",
    "    #6371*(2 * math.asin(math.sqrt(math.sin((math.radians(lat2)-math.radians(lat1))/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin((math.radians(lon2)-math.radians(lon1))/2)**2)))\n",
    "\n",
    "    return numpy.round(km_distance*0.621371)\n",
    "\n",
    "def extract_postcodes_api():\n",
    "    url = \"https://api.postcodes.io/postcodes\"\n",
    "    with open('Postcode - Copy of JSON Paramater Packaging.csv', newline='') as csvfile:\n",
    "        postcodes = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        for row in postcodes:\n",
    "            print(\"Row = \" + str(row))\n",
    "            #response = requests.post(url,data = row)\n",
    "            #json_output = json.loads(response)\n",
    "            #print(response.text)\n",
    "            \n",
    "def extract_postcodes_file():\n",
    "    long_lat_points = pandas.read_csv('Postcode - Latitude With No Nulls.csv',dtype='unicode')\n",
    "    return long_lat_points\n",
    "\n",
    "def print_only_completed_results():\n",
    "  df = pandas.read_csv('/Active Campaign - Contacts Report.csv',dtype='unicode')\n",
    "  print(len(df))\n",
    "  print(len(df.columns))\n",
    "\n",
    "  # Select Only Desired Columns\n",
    "  df2_only_desired_columns = df.iloc[:,[0,1,2,3,12,17,19,25,52,54,60,79,100]]\n",
    "  # Drop Any Columns with Any Missing Rows\n",
    "  return df2_only_desired_columns.dropna()\n",
    "\n",
    "def print_only_complete_records():\n",
    "  df = pandas.read_csv('/Active Campaign - Contacts Report.csv')\n",
    "  print(len(df))\n",
    "  print(len(df.columns))\n",
    "\n",
    "  # Select Only Desired Columns\n",
    "  df2_only_desired_columns = df.iloc[:,[0,1,2,3,12,17,19,25,52,54,60,79,100]]\n",
    "  print(df2_only_desired_columns.columns)\n",
    "  # Drop Any Columns with Any Missing Rows\n",
    "  return df2_only_desired_columns.dropna()\n",
    "\n",
    "def produce_data_frame():\n",
    "  df = pandas.read_csv('Active Campaign - Contacts Report.csv')\n",
    "  print(len(df))\n",
    "  print(len(df.columns))\n",
    "\n",
    "  # # Tapi.postcodes.io/postcodes/\n",
    "  #   url = https://wimbletech.api-us1.com/api/3/contacts?api_key=03607701a84655ffeffb618671d5f4be937c574b9bf1fe11ad19b157b1aef835d90768f0&include=fieldValues\n",
    "  # field_values_url = \"https://api.postcodes.io/postcodes/\"\n",
    "  # headers = {\"Accept\": \"application/json\",\"Api-Token\":\"03607701a84655ffeffb618671d5f4be937c574b9bf1fe11ad19b157b1aef835d90768f0\"}\n",
    "  # querystring = {\"status\":\"-1\",\"limit\": records_per_query,\"offset\": records_per_query*counter}\n",
    "  # response = requests.request(\"GET\", field_values_url, headers=headers, params=querystring)\n",
    "  # contacts_output = json.loads(response.text)[\"contacts\"]\n",
    "  \n",
    "    \n",
    "  # Select Only Desired Columns from the Database\n",
    "  df2_only_desired_columns = df.iloc[:,[0,1,2,3,12,17,19,25,52,54,60,79,100]]\n",
    "  \n",
    "  # Create Value Counts For Each Site\n",
    "  df2_site_counts = df2_only_desired_columns['*Venue - Site (final)'].value_counts().to_frame()\n",
    "  print(df2_site_counts.columns)\n",
    "\n",
    "  # Merge Aggregated Count Table with Original Table\n",
    "  df3_merged_table = df2_only_desired_columns.merge(df2_site_counts,how='outer',left_on='*Venue - Site (final)',right_index=True)\n",
    "  \n",
    "  # Merge Post-Codes On To Table\n",
    "  df4_postcode_included_table = df3_merged_table.merge(postcode_table,how='outer',left_on='*Venue - Site (final)',right_on=\"SiteName\")\n",
    "\n",
    "  # Log \n",
    "\n",
    "  # Insert Flag Columns for Null Values\n",
    "  df4_postcode_included_table['LastDateIncluded'] = df4_postcode_included_table['*Last Date'].notnull()\n",
    "  df4_postcode_included_table['StartDateIncluded'] = df4_postcode_included_table['*Start Date'].notnull()\n",
    "  \n",
    "  txt = \"HP52DA\"\n",
    "  x = re.search(\"^([A-Z]{1,2}[0-9]{1,2}[A-Z]{0,1}\\s{0,1}[0-9]{1}[A-Z]{2})$\", txt)\n",
    "\n",
    "  if x:\n",
    "      print(\"YES! We have a match!\")\n",
    "  else:\n",
    "      print(\"No match\")\n",
    "        \n",
    "  print(\"Regex = \" + str(re.search(\"^([A-Z]{1,2}[0-9]{1,2}[A-Z]{0,1}\\s{0,1}[0-9]{1}[A-Z]{2})$\", \"WD25 0JL\")))\n",
    "  df4_postcode_included_table['StringConvertedRegex'] = df4_postcode_included_table['*Postcode']\n",
    "  \n",
    "  df_long_lat_points = pandas.read_csv('Postcode - Latitude With No Nulls.csv',dtype='unicode')\n",
    "     \n",
    "#df4_postcode_included_table['ValidatedRegex'] = df4_postcode_included_table['*Postcode'] if re.search(\"^([A-Z]{1,2}[0-9]{1,2}[A-Z]{0,1}\\s{0,1}[0-9]{1}[A-Z]{2})$\", df4_postcode_included_table['*Postcode']) else \"Invalid\"\n",
    "#   df4_postcode_included_table['ValidatedRegex'] = re.match(postcode_regex_filter,str(df4_postcode_included_table['*Postcode']) if str(df4_postcode_included_table['*Postcode']) else \"Invalid\")\n",
    "\n",
    "  df4_postcode_included_table['VenueSiteFinalIncluded'] = df4_postcode_included_table['*Venue - Site (final)'].notnull()\n",
    "  df4_postcode_included_table['PostcodeIncluded'] = df4_postcode_included_table['*Postcode'].notnull()\n",
    "\n",
    "  # Could Add In Prospect / Member Live Field If Possible\n",
    "  # Write Search Queries That Return Filtered Fields for Dates Within Different Time Regions < 30 days >\n",
    "\n",
    "  # Output the file\n",
    "  df4_postcode_included_table.filter(items=[\"SitePostcode\",\"*Postcode\"])\n",
    "    \n",
    "  long_lat_points = pandas.read_csv('Postcode - Latitude With No Nulls.csv',dtype='unicode')[[\"Postcode\",\"Latitude\",\"Longitude\"]]\n",
    "  site_long_lat_data = pandas.read_csv('Postcode - OUTPUT - Site Latitude and Longtitude.csv',dtype='unicode')[[\"Postcode\",\"Latitude\",\"Longitude\"]]\n",
    "    \n",
    "\n",
    "  df_5_member_longlat_included = df4_postcode_included_table.merge(long_lat_points,how='left',left_on='*Postcode',right_on=\"Postcode\")\n",
    "  df_6 = df_5_member_longlat_included.merge(site_long_lat_data,how='left',left_on='SitePostcode',right_on=\"Postcode\")\n",
    "\n",
    "  # Drop Any Columns with Any Missing Rows\n",
    "  df_6[\"km_between_points\"] = df_6.apply(lambda x : calculate_km_distance_between_points(float(x[\"Latitude_x\"]),float(x[\"Longitude_x\"]),float(x[\"Latitude_y\"]),float(x[\"Longitude_y\"])),axis=1)\n",
    "  df_6[\"miles_between_points\"] = df_6.apply(lambda x : calculate_miles_distance_between_points(float(x[\"Latitude_x\"]),float(x[\"Longitude_x\"]),float(x[\"Latitude_y\"]),float(x[\"Longitude_y\"])),axis=1)\n",
    "  df_6.rename(columns={\"Postcode_x\": \"SitePostcode_x\" , \"Latitude_x\": \"SiteLatitude_x\",\"Longitude_x\":\"SiteLongitude_x\",\"Postcode_y\":\"HomePostcode_y\",\"Latitude_y\":\"Home_Latitude_y\",\"Longitude_y\":\"Home_Longitude_y\"})\n",
    "  \n",
    "  df_6.to_csv('ZohoOutput' + str(time.time()) + \".csv\",header = True,index=False)\n",
    "  return df_6\n",
    "\n",
    "def printout_contacts_report(dataframe):\n",
    "  df = pandas.read_csv('/Active Campaign - Contacts Report.csv',dtype='unicode')\n",
    "  print(len(df))\n",
    "  print(len(df.columns))\n",
    "\n",
    "  # Select Only Desired Columns\n",
    "  df2_only_desired_columns = df.iloc[:,[0,1,2,3,12,17,19,25,52,54,60,79,100]].sort_values(['*Last Date'])\n",
    "  # Create Additional Flag Fields Indicating Nan Values\n",
    "  df2_nan_flags = df2_only_desired_columns.isna()\n",
    "\n",
    "  return df2_only_desired_columns\n",
    "  # Conduct A Grouped Count Of How Many Members Are In Samples For Each Column (After Excluding Nulls)\n",
    "    # Where Start Date Is Before Today and L\n",
    "  above_35 = dataframe[dataframe[\"*Start Date\"] > 35]\n",
    "\n",
    "  # Conduct A Grouped Count Of How Many Members Are In Samples For Each Column (Non-Nulls Group By Sites)\n",
    "\n",
    "  # Conduct A Count Of Start Dates In The Future\n",
    "    # 1. List Of New Starter Members - Where Start Date Is After Today\n",
    "    # 2. Count Of New Starter Members By Site - Where Start Date Is After Today Grouped By Site\n",
    "  # Conduct A Count of Start Dates In The Last 30 Days\n",
    "    # 3. Count Of New Starter Members By Site - Where the Start Date Is Within the Last 30 Days\n",
    "    # 4. Count Of New Starter Members By Site - Where the Start Date Is Within The Last 30 Days - Grouped By Site\n",
    "  # Conduct A Count of Leaving Dates In The Next 30 Days\n",
    "    # 5. Count Of New Starter Members By Site - Where the Leave Date Is Within The Next 30 Days\n",
    "    # 6. Count Of New Starter Members By Site - Where the Start Date Is Within The Next 30 Days - Grouped By Site\n",
    "  # Conduct A Count of Start Dates In The Pass 90 Days\n",
    "    # 7. Count Of New Starter Members By Site - Where the Start Date Is Within The Next 90 Days\n",
    "    # 8. Count Of New Starter Members By Site - Where the Start Date Is Within The Next 90 Days - Grouped By Site=\n",
    "  \n",
    "  # Historical Data Analytics\n",
    "  # Membership Duration Time - Length Of Stay\n",
    "\n",
    "  # Conduct A Count of Postcode \n",
    "print_only_completed_results()\n",
    "#produce_data_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec620e6",
   "metadata": {},
   "source": [
    "### Redundant Code Below is Subject to Clarifications from the Active Campaign Develop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_all_contacts(global_list_object):\n",
    "  url = \"https://wimbletech.api-us1.com/api/3/contacts\"\n",
    "  for counter in range(number_of_contacts/100):\n",
    "    headers = {\"Accept\": \"application/json\",\"Api-Token\":\"03607701a84655ffeffb618671d5f4be937c574b9bf1fe11ad19b157b1aef835d90768f0\"}\n",
    "    querystring = {\"status\":\"-1\",\"limit\": records_per_query,\"offset\": records_per_query*counter}\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    contacts_output = json.loads(response.text)[\"contacts\"]\n",
    "    print(querystring)\n",
    "    global_list_object = global_list_object + contacts_output\n",
    "    \n",
    "    #Introduce Rate Limits\n",
    "    if(counter+1 % 5 == 0):\n",
    "      print(\"Pause for Rate-Limit\")\n",
    "      sleep(2)\n",
    "      console.log(\"Pause for Rate-Limit\")\n",
    "\n",
    "    # Write To File\n",
    "  with open('names.csv', 'w', newline='') as csvfile:\n",
    "    print(\"Field Names = \" + str(contacts_db[0].keys()))\n",
    "    fieldnames = contacts_output[0].keys()\n",
    "\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for item in contacts_db:\n",
    "      writer.writerow(item)\n",
    "      sleep(0.5)\n",
    "\n",
    "def retrieve_all_field_values(global_list_object):\n",
    "  for counter in range(number_of_contacts/100):\n",
    "    field_values_url = \"https://wimbletech.api-us1.com/api/3/fieldValues\"\n",
    "    headers = {\"Accept\": \"application/json\",\"Api-Token\":\"03607701a84655ffeffb618671d5f4be937c574b9bf1fe11ad19b157b1aef835d90768f0\"}\n",
    "    querystring = {\"status\":\"-1\",\"limit\": records_per_query,\"offset\": records_per_query*counter}\n",
    "    response = requests.request(\"GET\", field_values_url, headers=headers, params=querystring)\n",
    "    contacts_output = json.loads(response.text)[\"contacts\"]\n",
    "    print(querystring)\n",
    "    global_list_object = global_list_object + contacts_output\n",
    "      \n",
    "    #Introduce Rate Limits\n",
    "    if(counter+1 % 5 == 0):\n",
    "      print(\"Pause for Rate-Limit\")\n",
    "      sleep(2)\n",
    "      console.log(\"Pause for Rate-Limit\")\n",
    "\n",
    "    # Write To File\n",
    "    with open('fieldnames.csv', 'w', newline='') as csvfile:\n",
    "      print(\"Field Names = \" + str(contacts_db[0].keys()))\n",
    "      fieldnames = contacts_output[0].keys()\n",
    "\n",
    "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "      writer.writeheader()\n",
    "\n",
    "      for item in contacts_db:\n",
    "        writer.writerow(item)\n",
    "        sleep(0.5)\n",
    "\n",
    "def retrieve_field_values(id):\n",
    "  url = \"https://wimbletech.api-us1.com/api/3/fieldValues/\" + str(id)\n",
    "  headers = {\"Accept\": \"application/json\"}\n",
    "  headers = {\"Accept\": \"application/json\",\"Api-Token\":\"03607701a84655ffeffb618671d5f4be937c574b9bf1fe11ad19b157b1aef835d90768f0\"}\n",
    "  querystring = {\"status\":\"-1\",\"limit\": records_per_query,\"offset\": records_per_query*counter}\n",
    "  response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "  print(response.text)\n",
    "\n",
    "def retrieve_field_values_2():\n",
    "  url = \"https://wimbletech.api-us1.com/api/3/fields\"\n",
    "  querystring = {\"limit\":\"1\"}\n",
    "  headers = {\"Accept\": \"application/json\",\"Api-Token\":\"03607701a84655ffeffb618671d5f4be937c574b9bf1fe11ad19b157b1aef835d90768f0\"}\n",
    "  response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "  print(response.text)\n",
    "\n",
    "def retrieve_all_lists():\n",
    "  url = \"https://wimbletech.api-us1.com/api/3/lists\"\n",
    "  headers = {\"Accept\": \"application/json\"}\n",
    "  response = requests.request(\"GET\", url, headers=headers)\n",
    "  print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a4017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1862d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
